# Wikidata Radius Dumper - Implementation Plan

## Project Overview
A Next.js-based application that safely crawls Wikidata through WDQS (Wikidata Query Service) and exports RDF data in N-Triples (.nt) and Turtle (.ttl) formats.

## Status: âœ… Complete with Enhanced Features

### âœ… Completed Tasks
- [x] Create implementation plan document
- [x] Initialize Next.js project with TypeScript
- [x] Set up environment configuration
- [x] Create WDQS client with safety features
- [x] Implement MediaWiki API client
- [x] Build CONSTRUCT query generators
- [x] Create dump orchestration logic
- [x] Set up API routes
- [x] Build React UI components
- [x] Add documentation
- [x] Implement persistent dumps storage with timestamps
- [x] Add dump history management API
- [x] Create interactive RDF graph visualization
- [x] Add TTL parser for graph data extraction

### ðŸŽ‰ New Features Added
1. **Persistent Dumps Storage**: All dumps are now saved in timestamped directories under `/dumps` with metadata tracking
2. **Dump History API**: Browse and manage previous dumps through `/api/history` endpoint
3. **Interactive Graph Visualization**: View TTL files as interactive graphs using Cytoscape.js with node expansion and filtering

### ðŸ“‹ Next Steps
- Run `npm run dev` to start the development server
- Test with a small class (e.g., Q7889 for video games with max 10 instances)
- Dumps are now saved in `/dumps` folder with timestamps
- Use the graph visualization feature to explore RDF data interactively

---

## Architecture Overview

**Tech Stack:**
- **Next.js 14** (App Router) with TypeScript
- **Server-side crawling** to avoid CORS issues
- **Streaming exports** to handle large datasets
- **Rate-limited WDQS queries** with proper User-Agent headers
- **MediaWiki API** for efficient instance enumeration (avoiding SPARQL pagination)

## Key Features

1. **Safe WDQS Usage:**
   - Custom User-Agent headers per Wikimedia policy
   - Rate limiting (1-2 concurrent queries max)
   - Exponential backoff on 429/503 errors
   - 60-second query timeout enforcement
   - Retry-After header respect

2. **Efficient Crawling Strategy:**
   - Use MediaWiki's `haswbstatement` search instead of SPARQL OFFSET
   - Batch CONSTRUCT queries with VALUES clauses
   - Stream results directly to .nt files
   - Convert to .ttl at the end

3. **Radius Semantics:**
   - R=1: All truthy edges from instances + labels
   - R=2: All truthy edges from R=1 neighbors
   - Optional subclass expansion via P279*

## Project Structure

```
Wikidata_dumper/
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ next.config.js
â”œâ”€â”€ .env.local
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”œâ”€â”€ page.tsx
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ start/route.ts      # Start dump job
â”‚       â”œâ”€â”€ progress/route.ts   # SSE progress stream
â”‚       â”œâ”€â”€ download/route.ts   # File downloads
â”‚       â”œâ”€â”€ history/route.ts    # Dump history management
â”‚       â””â”€â”€ visualize/route.ts  # TTL to graph conversion
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ wdqsClient.ts          # SPARQL client with safety features
â”‚   â”œâ”€â”€ mwapi.ts               # MediaWiki API for instance enumeration
â”‚   â”œâ”€â”€ construct.ts           # CONSTRUCT query builders
â”‚   â”œâ”€â”€ subclasses.ts          # Subclass expansion
â”‚   â”œâ”€â”€ dump.ts                # Main orchestration logic
â”‚   â””â”€â”€ jobManager.ts          # Job queue management
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ types.ts               # TypeScript interfaces
â”‚   â”œâ”€â”€ schemas.ts             # Zod validation schemas
â”‚   â””â”€â”€ utils.ts               # Helper functions
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ DumpForm.tsx           # Configuration form
â”‚   â”œâ”€â”€ ProgressDisplay.tsx    # Live progress with ETA
â”‚   â”œâ”€â”€ DownloadLinks.tsx      # Result downloads
â”‚   â””â”€â”€ GraphViewer.tsx        # Interactive RDF visualization
â”œâ”€â”€ types/
â”‚   â””â”€â”€ cytoscape-fcose.d.ts  # Type declarations
â””â”€â”€ dumps/                     # Timestamped dump directories (git-ignored)
    â””â”€â”€ 2025-09-26T13-45-00_Q3305213/
        â”œâ”€â”€ metadata.json      # Dump configuration and status
        â”œâ”€â”€ dump.nt           # N-Triples format
        â””â”€â”€ dump.ttl          # Turtle format
```

## Implementation Phases

### Phase 1: Project Setup âœ…
- [x] Initialize Next.js with TypeScript
- [x] Install dependencies: `p-queue`, `n3`, `zod`, `date-fns`
- [x] Configure environment variables
- [x] Set up exports directory

### Phase 2: Server Modules âœ…
- [x] WDQS client with proper headers and rate limiting
- [x] MediaWiki API client for haswbstatement search
- [x] CONSTRUCT query builders for R=1 and R=2
- [x] Subclass expansion query

### Phase 3: Dump Orchestration âœ…
- [x] Job manager with progress tracking
- [x] Phase A: Enumerate instances via MediaWiki API
- [x] Phase B: Estimate total triples (sampling)
- [x] Phase C: Fetch R=1 data in batches
- [x] Phase D: Fetch R=2 data from neighbors
- [x] Phase E: Convert .nt to .ttl

### Phase 4: API Routes âœ…
- [x] POST /api/start - Start dump job
- [x] GET /api/progress - SSE stream for live updates
- [x] GET /api/download - Serve generated files

### Phase 5: UI Components âœ…
- [x] Configuration form with sensible defaults
- [x] Live progress display with ETA
- [x] Download links for .nt and .ttl

## SPARQL Query Templates

### Subclass Expansion
```sparql
SELECT ?c WHERE { ?c wdt:P279* wd:Q3305213 }
```

### R=1 CONSTRUCT (truthy edges + labels)
```sparql
PREFIX wd:  <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

CONSTRUCT {
  ?s ?p ?o .
  ?s rdf:type ?class .
  ?s rdfs:label ?sLabel .
  ?p rdfs:label ?pLabel .
  ?o rdfs:label ?oLabel .
  ?class rdfs:label ?classLabel .
}
WHERE {
  VALUES ?s { wd:Q42 wd:Q571 ... }
  {
    ?s ?p ?o .
    FILTER(STRSTARTS(STR(?p), STR(wdt:)))
  }
  UNION { ?s wdt:P31 ?class . }
  SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
}
```

### R=2 CONSTRUCT (neighbors)
Same as R=1 but VALUES gets first-hop object QIDs

## Safety Measures

1. **Rate Limiting:**
   - Max 2 concurrent WDQS queries
   - 200ms minimum delay between requests
   - Exponential backoff on errors

2. **Error Handling:**
   - Graceful timeout handling
   - Retry logic with backoff
   - Progress persistence for resume capability

3. **Resource Management:**
   - Stream processing for large datasets
   - Chunked file writing
   - Memory-efficient neighbor tracking

## Default Configuration

- **Class:** Q3305213 (painting)
- **Radius:** 2
- **Max Instances:** 10,000 (configurable)
- **Language:** English
- **Include Subclasses:** true
- **Graph:** Truthy (wdt:) by default

## Environment Variables

```env
WDQS_ENDPOINT=https://query.wikidata.org/sparql
WIKIMEDIA_API=https://www.wikidata.org/w/api.php
AGENT_NAME=QrakenWDRadius/0.1 (contact: you@example.com)
DEFAULT_LANG=en
```

## Dependencies

- **next**: ^14.0.0
- **react**: ^18.0.0
- **typescript**: ^5.0.0
- **p-queue**: ^8.0.0 (rate limiting)
- **n3**: ^1.17.0 (RDF serialization)
- **zod**: ^3.22.0 (input validation)
- **date-fns**: ^3.0.0 (date utilities)

## Notes

- WDQS truthy graph uses `wdt:` predicates (fast & compact)
- Full statement graph (`p:/ps:/pq:`) available but not default
- Labels fetched via SERVICE wikibase:label
- Paintings live in main graph (query.wikidata.org)
- No scholarly articles needed (post-split endpoint)
